{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSession\n",
    "Objetivo: crear una Spark session, entrada a las funcionalidades de spark. \n",
    "Con base en el video y en el repositorio de Errodringer, pero con la adecuación y adaptación a mi version de pyspark y python. \n",
    "\n",
    "https://github.com/errodringer/CursoBigData_PySpark/tree/master\n",
    "https://www.youtube.com/playlist?list=PLYFBiuYObvKB_k0bnkI41biMjD12t44GN\n",
    "\n",
    "Nota: para iniciar el jupyter notebook como en el vídeo, yo en mi consola puse el siguiente comando porque no me lo reconoce. \n",
    "\n",
    "python -m notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark \n",
    "findspark.init()\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import col, split\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName ('firstSession').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las configuraciones disponibles en Spark:\n",
    "https://spark.apache.org/docs/latest/configuration.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://NTTD-F2779S3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>firstSession</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x137f6fcb500>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame\n",
    "\n",
    "En Python, un DataFrame es una estructura de datos bidimensional que se encuentra en la biblioteca pandas. Está diseñado para almacenar y manipular datos de manera similar a una hoja de cálculo o una tabla de base de datos relacional. \n",
    "No me dejo crear el df a partir de una lista si no importaba pandas, busque los mil errores y esta fue una solición. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear Tabla\n",
    "a) A partir de una lista \n",
    "\n",
    "En este caso se utilizo pandas aunque no es lo más recomendable porque se utilizan más recursos de la computadora, por lo que tendríamos procesamientos más lentos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas = [\"id\", \"nombre\", \"letra\"]\n",
    "lista_1 = [(1, \"Errodringer\", \"a\"), (2, \"Paco\", \"b\"), (3, \"Hola\", \"c\"), (4, \"Adios\", \"d\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>nombre</th>\n",
       "      <th>letra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Errodringer</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Paco</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Hola</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Adios</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       nombre letra\n",
       "0   1  Errodringer     a\n",
       "1   2         Paco     b\n",
       "2   3         Hola     c\n",
       "3   4        Adios     d"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.DataFrame(lista_1, columns= columnas)\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) A partir de un CSV o txt \n",
    "\n",
    "Es uno de los formatos más comunes, de manera general es df = spark.read.formato(path) aunque no son los únicos formatos ni las unicas variantes. Con el .csv me toma las primeras dos lineas como el header y en el .txt no reconoce el header. \n",
    "\n",
    "Para este ejemplo al leer el csv y mostrarlo con show, se observa que coloca los datos en una sóla columna, por lo que se utiliza la funcion .withColumn para reesctibir el schema. Se probaron otras formas de inferir el schema y hacer directamente un split pero ninguna funciono a que el resultado fuera, dos columnas con encabezado.Los numeros de la  función withColumn indican las posiciones que toma la columna que se crea y toma en cuenta los espacios. \n",
    "\n",
    "\n",
    "En el caso que se encuentren separados por comas tendría que buscar el comando que reconozca esta separación, por el momento me manejo sólamente por posiciones y espacios de columna. \n",
    "\n",
    "Coloque el .drop directamente al nombrar las columnas aunque se puede poner fuera. Simplemente corta y elimina la columna de los datos crudos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- microns  transmission: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = spark.read.csv(\"data\\\\DBS_LW_ModA_highres.txt\", header=True, inferSchema=True )\n",
    "df_2.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- microns: string (nullable = true)\n",
      " |-- transmission: string (nullable = true)\n",
      "\n",
      "+-------+------------+\n",
      "|microns|transmission|\n",
      "+-------+------------+\n",
      "|2      | 1.93996E-05|\n",
      "|2.001  | 1.79938E-05|\n",
      "|2.002  | 1.67365E-05|\n",
      "|2.003  | 1.56101E-05|\n",
      "|2.004  | 1.45996E-05|\n",
      "|2.005  | 1.36915E-05|\n",
      "|2.006  | 1.28742E-05|\n",
      "|2.007  | 1.21374E-05|\n",
      "|2.008  | 1.14722E-05|\n",
      "|2.009  | 1.08703E-05|\n",
      "|2.01   | 1.03247E-05|\n",
      "|2.011  | 9.82864E-06|\n",
      "|2.012  | 9.37634E-06|\n",
      "|2.013  | 8.96236E-06|\n",
      "|2.014  | 8.58179E-06|\n",
      "|2.015  | 8.23018E-06|\n",
      "|2.016  | 7.90358E-06|\n",
      "|2.017  | 7.59852E-06|\n",
      "|2.018  | 7.31207E-06|\n",
      "|2.019  | 7.04185E-06|\n",
      "|2.02   | 6.78604E-06|\n",
      "|2.021  | 6.54336E-06|\n",
      "|2.022  | 6.31297E-06|\n",
      "|2.023  | 6.09444E-06|\n",
      "|2.024  | 5.88758E-06|\n",
      "|2.025  | 5.69239E-06|\n",
      "|2.026  | 5.50893E-06|\n",
      "|2.027  | 5.33726E-06|\n",
      "|2.028  | 5.17737E-06|\n",
      "|2.029  | 5.02919E-06|\n",
      "|2.03   | 4.89252E-06|\n",
      "|2.031  | 4.76713E-06|\n",
      "|2.032  | 4.65268E-06|\n",
      "|2.033  | 4.54879E-06|\n",
      "|2.034  | 4.45506E-06|\n",
      "|2.035  | 4.37107E-06|\n",
      "|2.036  |  4.2964E-06|\n",
      "|2.037  | 4.23064E-06|\n",
      "|2.038  | 4.17341E-06|\n",
      "|2.039  | 4.12436E-06|\n",
      "|2.04   | 4.08316E-06|\n",
      "|2.041  | 4.04954E-06|\n",
      "|2.042  | 4.02324E-06|\n",
      "|2.043  | 4.00406E-06|\n",
      "|2.044  | 3.99182E-06|\n",
      "|2.045  |  3.9864E-06|\n",
      "|2.046  |  3.9877E-06|\n",
      "|2.047  | 3.99567E-06|\n",
      "|2.048  | 4.01028E-06|\n",
      "|2.049  | 4.03156E-06|\n",
      "+-------+------------+\n",
      "only showing top 50 rows\n",
      "\n",
      "Número de columnas: 2\n"
     ]
    }
   ],
   "source": [
    "df_2 = df_2.withColumn(\"microns\", F.substring(F.col(\"microns  transmission\"),0,7))\\\n",
    "    .withColumn(\"transmission\", F.substring(F.col(\"microns  transmission\"),7,15))\\\n",
    "    .drop(\"microns  transmission\")\n",
    "df_2.printSchema()\n",
    "df_2.count()\n",
    "df_2.first()\n",
    "df_2.show(50)\n",
    "df_2.collect()\n",
    "nombres_columnas = df_2.columns\n",
    "num_columns = len(df_2.columns)\n",
    "print(\"Número de columnas:\", num_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|microns|transmission|\n",
      "+-------+------------+\n",
      "|3.004  | 0.935388099|\n",
      "|3.005  | 0.935807141|\n",
      "|3.006  | 0.936212417|\n",
      "|3.007  | 0.936603099|\n",
      "|3.008  | 0.936978404|\n",
      "|3.05   | 0.936888018|\n",
      "|3.051  | 0.936645444|\n",
      "|3.052  | 0.936403758|\n",
      "|3.053  | 0.936163889|\n",
      "|3.054  | 0.935926744|\n",
      "|3.055  | 0.935693212|\n",
      "|3.056  | 0.935464161|\n",
      "|3.057  | 0.935240431|\n",
      "|3.058  | 0.935022836|\n",
      "|3.094  | 0.935184363|\n",
      "|3.095  | 0.935431772|\n",
      "|3.096  | 0.935689107|\n",
      "|3.097  | 0.935955849|\n",
      "|3.098  | 0.936231463|\n",
      "|3.099  | 0.936515399|\n",
      "+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.filter((col('microns') > 2) & (col('transmission') .between(0.935, 0.937))\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
